{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73491958",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b68b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Import embeddings_df\n",
    "constitution = pd.read_csv(\"constitution_embeddings.csv\")\n",
    "\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "constitution[\"embedding\"] = constitution[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts- this is later needed once we find those embedding indices with top similarity to the input to map back to the text.\n",
    "constitution_and_chunks = constitution.to_dict(orient=\"records\") # each row becomes a dicttionary and all these becomes a list of dicts. records => each row is a dictionary\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(constitution[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "constitution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cafbab",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the model\n",
    "from sentence_transformers import util, SentenceTransformer #util is for dot product\n",
    "\n",
    "# embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",device = device)\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"multi-qa-mpnet-base-dot-v1\",device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70073478",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"elections\"\n",
    "print(f'Query : {query}')\n",
    "\n",
    "#embed the query\n",
    "q_embed = embedding_model.encode(query,convert_to_tensor = True)\n",
    "\n",
    "#get similarity scores with dot product\n",
    "##to time this \n",
    "from time import perf_counter as timer \n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a= q_embed,b= embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f'Time taken to get scores on {len(embeddings)} embeddings : {end_time-start_time:.5f} seconds.')\n",
    "\n",
    "#get top k=5 results\n",
    "top_results = torch.topk(dot_scores,k=5)\n",
    "top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f83540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "#whatever text you give , this with return a wrapped version of it\n",
    "def print_wrapped(text,wrap_length = 80)  :\n",
    "    wrapped_text = textwrap.fill(text,wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7612ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "\n",
    "#zip them together and loop over to print\n",
    "for score,idx in zip(top_results[0],top_results[1]) :\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(\"Text:\")\n",
    "    #get the text from dict and print as wrapped\n",
    "    print_wrapped(constitution_and_chunks[idx][\"text\"])\n",
    "    #print the part as well as well\n",
    "    print(f\"section : {constitution_and_chunks[idx][\"section\"]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04ed7d",
   "metadata": {},
   "source": [
    "## Functionizing the semantic search pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ceea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 constitution_and_chunks: list[dict]=constitution_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Since constitution_and_chunks is a list of dictionaries, it requires constitution_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant text chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(constitution_and_chunks[idx][\"text\"])\n",
    "        #print the part as well as well\n",
    "        print(f\"section : {constitution_and_chunks[idx][\"section\"]}\")\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac428b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the function-1\n",
    "query = \"panchayat\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d833315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function-2 : Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1258e",
   "metadata": {},
   "source": [
    "## LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eff3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66da46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = None\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42547b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# --- Settings ---\n",
    "model_id = \"google/gemma-2b-it\"   # instruction-tuned 2B\n",
    "attn_implementation = \"eager\"     # or \"sdpa\" if your stack supports it\n",
    "\n",
    "# 4-bit quantization config (saves VRAM)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # compute in fp16\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Loading {model_id} in 4-bit...\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Model (device_map='auto' places weights on GPU/CPU as needed)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=False,\n",
    "    attn_implementation=attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model.device ,llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"how is the vice president elected and what role does the parliament in electing the vice president?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ee35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig \n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeed8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format better \n",
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [\n",
    "    \"What does the Preamble of the Indian Constitution declare about justice, liberty, equality, and fraternity?\",\n",
    "    \"How can Parliament form new states or alter the boundaries of existing states under Part I?\",\n",
    "    \"What were the provisions regarding citizenship at the commencement of the Constitution?\",\n",
    "    \"Which Fundamental Rights are guaranteed under the Right to Equality?\",\n",
    "    \"What Directive Principles guide the State in securing equal pay for equal work?\",\n",
    "    \"Why is the Preamble considered the soul of the Constitution of India?\",\n",
    "    \"How do Articles 5 to 11 reflect the framers' approach towards citizenship?\",\n",
    "    \"In what ways do Fundamental Rights ensure limitations on the power of the State?\",\n",
    "    \"How are Fundamental Duties different from Directive Principles of State Policy?\",\n",
    "    \"What role does Article 32 play in making the Fundamental Rights enforceable?\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a85d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "query = random.choice(query_list)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ce3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"text\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Make sure your answers are clear, detailed, and explanatory, using examples from the Constitution wherever possible.\n",
    "    Use the following examples as reference for the ideal answer style.\n",
    "\n",
    "    Example 1:\n",
    "    Query: What does the Preamble of the Indian Constitution declare?\n",
    "    Answer: The Preamble declares India to be a Sovereign, Socialist, Secular, Democratic Republic. It secures to all citizens Justice—social, economic, and political; Liberty of thought, expression, belief, faith, and worship; Equality of status and opportunity; and Fraternity assuring the dignity of the individual and the unity and integrity of the Nation. It was adopted on 26 November 1949, reflecting the vision of the Constituent Assembly.\n",
    "\n",
    "    Example 2:\n",
    "    Query: How can new states be created or existing states altered under the Constitution?\n",
    "    Answer: Articles 2 and 3 empower Parliament to admit new states, establish states, or alter existing states’ boundaries, names, or areas. For such changes, a bill must be introduced on the recommendation of the President, and if it affects any state’s area, boundaries, or name, the President must refer it to the concerned state legislature for its views. However, Parliament is not bound to accept the state’s opinion, ensuring flexibility in India’s federal structure.\n",
    "\n",
    "    Example 3:\n",
    "    Query: What are Fundamental Rights, and why are they important?\n",
    "    Answer: Fundamental Rights, enshrined in Part III (Articles 12–35), guarantee essential freedoms like equality before law, freedom of speech, protection of life and liberty, and the right to constitutional remedies. They act as limitations on state power and safeguard individual dignity. For instance, Article 32 empowers citizens to directly approach the Supreme Court for enforcement of rights, making these provisions justiciable and enforceable.\n",
    "\n",
    "    Now use the following context items to answer the user query:\n",
    "    {context}\n",
    "\n",
    "    Relevant passages: <extract relevant passages from the context here>\n",
    "    User query: {query}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,#to get the output prompt of this function as a text and not a tokenized version\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "    \n",
    "# Create a list of context items\n",
    "context_items = [constitution_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)#this is why we gave tokenize = False in the prompt_formatter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt \n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58051397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7723c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836cd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bdb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0eb6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a36686f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "const_env (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
